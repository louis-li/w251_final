{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_style_transfer.ipynb","provenance":[{"file_id":"1QvW-7LSoBwWUVpHAp6K2OKJucUJgCsZY","timestamp":1605404845183}],"collapsed_sections":[],"authorship_tag":"ABX9TyO5iPoDm8/Cx4KQpvEVYLpd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JDwo5Kylx4Ca"},"source":["## Neural Style Transfer Implementation with PyTorch\n","\n","Applying the neural style transfer algorithm for generating footprint images with different weather and soil conditions.\n","\n","Source: https://github.com/rrmina/neural-style-pytorch"]},{"cell_type":"code","metadata":{"id":"EqSydY82iYVk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606574487160,"user_tz":-540,"elapsed":121139,"user":{"displayName":"Sang-Ki Nam","photoUrl":"","userId":"07448744021609013567"}},"outputId":"c202cf87-8b4e-43af-bfcb-c6d29563aad3"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import models, transforms\n","\n","import cv2\n","import copy\n","from PIL import Image\n","\n","import os\n","import time\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8t3embGbidv1"},"source":["# Hyperparameters\n","MAX_IMAGE_SIZE = 512\n","\n","# Optimizer\n","OPTIMIZER = 'adam' #or 'lbfgs'\n","ADAM_LR = 10\n","CONTENT_WEIGHT = 5e0 \n","STYLE_WEIGHT = 1e2   \n","TV_WEIGHT = 1e-3     \n","NUM_ITER = 500\n","SHOW_ITER = 100\n","\n","# Image Files\n","INIT_IMAGE = 'random' # or 'content'\n","PRESERVE_COLOR = 'True' # 'False'\n","PIXEL_CLIP = 'True' # or 'False' - Clipping produces better images\n","\n","style_file = \"style3.jpg\"\n","STYLE_PATH = \"/content/drive/My Drive/Colab Notebooks/w251/images/\" + style_file\n","\n","result_dir = \"/content/drive/My Drive/Colab Notebooks/w251/images/style3_result_amur_tiger1_43/\"\n","directory = \"/content/drive/My Drive/Colab Notebooks/w251/images/training_amur_tiger1_43/\"\n","\n","\"\"\"\n","PRETRAINED VGG MODELS \n","GITHUB REPO: https://github.com/jcjohnson/pytorch-vgg\n","VGG 19: https://web.eecs.umich.edu/~justincj/models/vgg19-d01eb7cb.pth\n","VGG 16: https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth\n","\"\"\"\n","#VGG19_PATH = 'models/vgg19-d01eb7cb.pth'\n","VGG19_PATH = \"/content/drive/My Drive/Colab Notebooks/w251/models/vgg19-d01eb7cb.pth\"\n","POOL = 'max'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rMFoDPgji6h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606574572503,"user_tz":-540,"elapsed":575,"user":{"displayName":"Sang-Ki Nam","photoUrl":"","userId":"07448744021609013567"}},"outputId":"3142a207-7469-4aef-afa1-ee7334fac281"},"source":["# Print the device\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YRy03YqNjnRf"},"source":["# Utils\n","# Load image file\n","def load_image(path):\n","    # Images loaded as BGR\n","    img = cv2.imread(path)\n","    return img\n","\n","# Show image\n","def show(img):\n","    # Convert from BGR to RGB\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    \n","    # imshow() only accepts float [0,1] or int [0,255]\n","    img = np.array(img/255).clip(0,1)\n","    \n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(img)\n","    plt.show()\n","    \n","# Save Image as out{num_iterms}.png\n","def saveimg(img, iters):\n","    if (PIXEL_CLIP=='True'):\n","        img = img.clip(0, 255)\n","    cv2.imwrite('out'+str(iters)+'.png', img)\n","    \n","# Color transfer\n","def transfer_color(src, dest):\n","    if (PIXEL_CLIP=='True'):\n","        src, dest = src.clip(0,255), dest.clip(0,255)\n","        \n","    # Resize src to dest's size\n","    H,W,_ = src.shape \n","    dest = cv2.resize(dest, dsize=(W, H), interpolation=cv2.INTER_CUBIC)\n","    \n","    dest_gray = cv2.cvtColor(dest, cv2.COLOR_BGR2GRAY) #1 Extract the Destination's luminance\n","    src_yiq = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)   #2 Convert the Source from BGR to YIQ/YCbCr\n","    src_yiq[...,0] = dest_gray                         #3 Combine Destination's luminance and Source's IQ/CbCr\n","    \n","    return cv2.cvtColor(src_yiq, cv2.COLOR_YCrCb2BGR)  #4 Convert new image from YIQ back to BGR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0xThmS5j2On"},"source":["# Preprocessing\n","def itot(img):\n","    # Rescale the image\n","    H, W, C = img.shape\n","    image_size = tuple([int((float(MAX_IMAGE_SIZE) / max([H,W]))*x) for x in [H, W]])\n","    \n","    itot_t = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize(image_size),\n","        transforms.ToTensor()\n","    ])\n","    \n","    # Subtract the means\n","    normalize_t = transforms.Normalize([103.939, 116.779, 123.68],[1,1,1])\n","    tensor = normalize_t(itot_t(img)*255)\n","    \n","    # Add the batch_size dimension\n","    tensor = tensor.unsqueeze(dim=0)\n","    return tensor\n","\n","def ttoi(tensor):\n","    # Add the means\n","    ttoi_t = transforms.Compose([\n","        transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n","    \n","    # Remove the batch_size dimension\n","    tensor = tensor.squeeze()\n","    img = ttoi_t(tensor)\n","    img = img.cpu().numpy()\n","    \n","    # Transpose from [C, H, W] -> [H, W, C]\n","    img = img.transpose(1, 2, 0)\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyuUrIbjj43A"},"source":["# Preprocessing\n","def itot(img):\n","    # Rescale the image\n","    H, W, C = img.shape\n","    image_size = tuple([int((float(MAX_IMAGE_SIZE) / max([H,W]))*x) for x in [H, W]])\n","    \n","    itot_t = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize(image_size),\n","        transforms.ToTensor()\n","    ])\n","    \n","    # Subtract the means\n","    normalize_t = transforms.Normalize([103.939, 116.779, 123.68],[1,1,1])\n","    tensor = normalize_t(itot_t(img)*255)\n","    \n","    # Add the batch_size dimension\n","    tensor = tensor.unsqueeze(dim=0)\n","    return tensor\n","\n","def ttoi(tensor):\n","    # Add the means\n","    ttoi_t = transforms.Compose([\n","        transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n","    \n","    # Remove the batch_size dimension\n","    tensor = tensor.squeeze()\n","    img = ttoi_t(tensor)\n","    img = img.cpu().numpy()\n","    \n","    # Transpose from [C, H, W] -> [H, W, C]\n","    img = img.transpose(1, 2, 0)\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBpk53clj8PD"},"source":["# Load VGG19 Skeleton\n","vgg = models.vgg19(pretrained=False)\n","\n","# Load pretrained weights\n","vgg.load_state_dict(torch.load(VGG19_PATH), strict=False)\n","\n","# Change Pooling Layer\n","def pool_(model, pool='avg'):\n","    if (pool=='avg'):\n","        ct=0\n","        for layer in model.children():\n","            if isinstance(layer, nn.MaxPool2d):\n","                model[ct] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n","            ct+=1\n","    elif (pool=='max'):\n","        ct=0\n","        for layer in model.children():\n","            if isinstance(layer, nn.AvgPool2d):\n","                model[ct] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","            ct+=1\n","            \n","    return model\n","\n","#vgg.features = pool_(vgg.features, POOL)\n","\n","# Extract only the 'features' network, \n","# 'classifier' network is not needed\n","model = copy.deepcopy(vgg.features)\n","model.to(device)\n","\n","# Turn-off unnecessary gradient tracking\n","for param in model.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-b7HoCaYkAQv"},"source":["\"\"\"\n","Loss Functions\n","All loss functions are basically Mean of the Squared Errors (MSE)\n","g - generated features\n","c - content features\n","s - style features\n","\n","1. What is Total Variation(TV) Loss ???\n","    The total variation (TV) loss encourages spatial smoothness in the generated image. \n","   It was not used by Gatys et al in their CVPR paper but it can sometimes improve the \n","   results; for more details and explanation see Mahendran and Vedaldi \n","   \"Understanding Deep Image Representations by Inverting Them\" CVPR 2015.\n","\n","   - @jcjohnson\n","   https://github.com/jcjohnson/neural-style/issues/302\n","\n","2. How to implement TV Loss?\n","    https://en.wikipedia.org/wiki/Total_variation_denoising\n","\"\"\"\n","mse_loss = torch.nn.MSELoss()\n","def gram(tensor):\n","    B, C, H, W = tensor.shape\n","    x = tensor.view(C, H*W)\n","    return torch.mm(x, x.t())\n","\n","def content_loss(g, c):\n","    loss = mse_loss(g, c)\n","    return loss\n","    \n","def style_loss(g, s):\n","    c1,c2 = g.shape\n","    loss = mse_loss(g, s)\n","    return loss / (c1**2) # Divide by square of channels\n","\n","def tv_loss(c):\n","    x = c[:,:,1:,:] - c[:,:,:-1,:]\n","    y = c[:,:,:,1:] - c[:,:,:,:-1]\n","    loss = torch.sum(torch.abs(x)) + torch.sum(torch.abs(y))\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-i4lWZIGkFfx"},"source":["# VGG Forward Pass\n","def get_features(model, tensor):\n","    layers = {\n","        '3': 'relu1_2',   # Style layers\n","        '8': 'relu2_2',\n","        '17' : 'relu3_3',\n","        '26' : 'relu4_3',\n","        '35' : 'relu5_3',\n","        '22' : 'relu4_2', # Content layers\n","        #'31' : 'relu5_2'\n","    }\n","    \n","    # Get features\n","    features = {}\n","    x = tensor\n","    for name, layer in model._modules.items():\n","        x = layer(x)\n","        if name in layers:\n","            if (name=='22'):   # relu4_2\n","                features[layers[name]] = x\n","            elif (name=='31'): # relu5_2\n","                features[layers[name]] = x\n","            else:\n","                b, c, h, w = x.shape\n","                features[layers[name]] = gram(x) / (h*w)\n","                \n","            # Terminate forward pass\n","            if (name == '35'):\n","                break\n","            \n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaxYq9SSkJNR"},"source":["# Generate Initial Image\n","def initial(content_tensor, init_image='random'):\n","    B, C, H, W = content_tensor.shape\n","    if (init_image=='random'):\n","        tensor = torch.randn(C, H, W).mul(0.001).unsqueeze(0)\n","    else:\n","        tensor = content_tensor.clone().detach()\n","    \n","    return tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqZbKzR7kLrf"},"source":["def stylize(iteration=NUM_ITER):     \n","    # Get features representations/Forward pass\n","    content_layers = ['relu4_2']\n","    content_weights = {'relu4_2': 1.0} \n","    style_layers = ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3']\n","    style_weights = {'relu1_2': 0.2, 'relu2_2': 0.2, 'relu3_3': 0.2, 'relu4_3': 0.2, 'relu5_3': 0.2}\n","    c_feat = get_features(model, content_tensor)\n","    s_feat = get_features(model, style_tensor)\n","    \n","    i = [0]\n","    while i[0] < iteration:\n","        def closure():\n","            # Zero-out gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            g_feat = get_features(model, g)\n","\n","            # Compute Losses\n","            c_loss=0\n","            s_loss=0\n","            for j in content_layers:\n","                c_loss += content_weights[j] * content_loss(g_feat[j], c_feat[j])\n","            for j in style_layers:\n","                s_loss += style_weights[j] * style_loss(g_feat[j], s_feat[j])\n","            \n","            c_loss = CONTENT_WEIGHT * c_loss\n","            s_loss = STYLE_WEIGHT * s_loss\n","            t_loss = TV_WEIGHT * tv_loss(g.clone().detach())\n","            total_loss = c_loss + s_loss + t_loss\n","\n","            # Backprop\n","            total_loss.backward(retain_graph=True)\n","            \n","            # Print Loss, show and save image\n","            i[0]+=1\n","            if (((i[0] % SHOW_ITER) == 1) or (i[0]==NUM_ITER)):\n","                print(\"Style Loss: {} Content Loss: {} TV Loss: {} Total Loss : {}\".format(s_loss.item(), c_loss.item(), t_loss, total_loss.item()))\n","                if (PRESERVE_COLOR=='True'):\n","                    g_ = transfer_color(ttoi(content_tensor.clone().detach()), ttoi(g.clone().detach()))\n","                else:\n","                    g_ = ttoi(g.clone().detach())\n","                show(g_)\n","                saveimg(g_, i[0]-1)\n","                plt.show()\n","            \n","            return (total_loss)\n","        \n","        # Weight/Pixel update\n","        optimizer.step(closure)\n","\n","    return g"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhzGb_PPkQuZ"},"source":["def saveimg2(path, img, iters):\n","    if (PIXEL_CLIP=='True'):\n","        img = img.clip(0, 255)\n","    cv2.imwrite(path+'.png', img)\n","\n","def saveimg3(path, img, iters):\n","    if (PIXEL_CLIP=='True'):\n","        img = img.clip(0, 255)\n","    cv2.imwrite(path+'preserved.png', img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KNqLkKMkS5l"},"source":["for filename in os.listdir(directory):\n","    if filename.endswith(\".jpg\") or filename.endswith(\".JPG\"):\n","        print(\"starting \" + os.path.join(directory, filename))\n","        \n","        CONTENT_PATH = os.path.join(directory, filename)\n","        RESULT_PATH = result_dir + filename\n","        \n","        # Load Images\n","        content_img = load_image(CONTENT_PATH)\n","        style_img = load_image(STYLE_PATH) \n","        \n","        # Show Images\n","        show(content_img)\n","        show(style_img)\n","        \n","        # Convert Images to Tensor\n","        content_tensor = itot(content_img).to(device)\n","        style_tensor = itot(style_img).to(device)\n","        g = initial(content_tensor, init_image=INIT_IMAGE)\n","        g = g.to(device).requires_grad_(True)\n","        \n","        \"\"\"\n","        Define Optimizer\n","        The optimizer minimizes the total loss by updating the tensor 'g'.\n","        \"\"\"\n","        start = time.time()\n","\n","        if (OPTIMIZER=='lbfgs'):\n","            optimizer = optim.LBFGS([g])\n","        elif (OPTIMIZER=='adam'):\n","            optimizer = optim.Adam([g], lr=ADAM_LR)\n","\n","        print(\"\")\n","        print('stylizing!!')    \n","        # Stylize!\n","        out = stylize(iteration=NUM_ITER)\n","\n","        end = time.time()\n","        print(\"전체 소요 시간: {:.1f}\".format(end-start))\n","        \n","        \n","        # Convert Tesnsor to Images \n","        show(content_img) # Original Content\n","\n","        show(ttoi(g.clone().detach())) # Style Transfer\n","        saveimg(ttoi(g.clone().detach()), NUM_ITER)\n","\n","        if (PRESERVE_COLOR=='True'):\n","            c_clone = ttoi(content_tensor.clone().detach())\n","            g_clone = ttoi(g.clone().detach())\n","            g_preserve = transfer_color(c_clone, g_clone) # Style Transfer + Preserve original color\n","            show(g_preserve)\n","            saveimg(g_preserve, 333) # out333 = final with preseved colors     \n","        \n","        # Save stylized and preserved images \n","        saveimg2(RESULT_PATH, g_clone, 333)\n","        saveimg3(RESULT_PATH, g_preserve, 333)\n","        \n","        continue\n","    else:\n","        print(\"Opps! something is wrong in reading \"+ os.path.join(directory, filename))\n","        continue"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPUTlIoXkabD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606576639238,"user_tz":-540,"elapsed":2062439,"user":{"displayName":"Sang-Ki Nam","photoUrl":"","userId":"07448744021609013567"}},"outputId":"ba11bb39-6150-4d84-acd9-89546eb2e6fa"},"source":["print(\"Done!!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPUPevVT1oVB"},"source":[""],"execution_count":null,"outputs":[]}]}